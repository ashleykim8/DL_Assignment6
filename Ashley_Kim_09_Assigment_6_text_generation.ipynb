{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Copyright\n",
        "\n",
        "<PRE>\n",
        "Copyright (c) Bálint Gyires-Tóth - All Rights Reserved\n",
        "You may use and modify this code for research and development purpuses.\n",
        "Using this code for educational purposes (self-paced or instructor led) without the permission of the author is prohibited.\n",
        "</PRE>"
      ],
      "metadata": {
        "id": "CtuSrazlNYEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: RNN text generation with your favorite book\n"
      ],
      "metadata": {
        "id": "vriXNd_nL2q6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dataset\n",
        "- Download your favorite book from https://www.gutenberg.org/\n",
        "- Combine all sonnets into a single text source.  \n",
        "- Split into training (80%) and validation (20%).  "
      ],
      "metadata": {
        "id": "Q5atve1sMH9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download txt from webpage\n",
        "!wget https://www.gutenberg.org/cache/epub/67098/pg67098.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHjLdbnn6b_i",
        "outputId": "44fa65bd-b525-4596-800d-3eb7ee544b14"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-23 20:03:12--  https://www.gutenberg.org/cache/epub/67098/pg67098.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 153227 (150K) [text/plain]\n",
            "Saving to: ‘pg67098.txt.1’\n",
            "\n",
            "pg67098.txt.1       100%[===================>] 149.64K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-04-23 20:03:13 (1.90 MB/s) - ‘pg67098.txt.1’ saved [153227/153227]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: downloaded Winnie the Pooh (pg67098.txt)\n",
        "\n",
        "with open(\"pg67098.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  full_text = f.read()\n",
        "\n",
        "# clean the text of the gutenberg info\n",
        "\n",
        "# the txt of Winnie the Pooh has two starting markers,\n",
        "#   we want to start on the 2nd\n",
        "start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK WINNIE-THE-POOH ***\"\n",
        "end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK WINNIE-THE-POOH ***\"\n",
        "\n",
        "first_start_idx = full_text.find(start_marker)\n",
        "second_start_idx = full_text.find(start_marker, first_start_idx + 1)\n",
        "\n",
        "start_idx = second_start_idx + len(start_marker)\n",
        "end_idx = full_text.find(end_marker)\n",
        "\n",
        "book_text = full_text[start_idx:end_idx].strip()\n",
        "\n",
        "\n",
        "# split into 80% train, 20% val\n",
        "train_size = int(0.8 * len(book_text))\n",
        "train_data = book_text[:train_size]\n",
        "val_data = book_text[train_size:]"
      ],
      "metadata": {
        "id": "QvKdt5EyMDug"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Preprocessing\n",
        "- Convert text to lowercase.  \n",
        "- Remove punctuation (except basic sentence delimiters).  \n",
        "- Tokenize by words or characters (your choice).  \n",
        "- Build a vocabulary (map each unique word to an integer ID)."
      ],
      "metadata": {
        "id": "4eQMcyPgMLJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lC5Ei4muTH8X",
        "outputId": "7d85f0d5-cde8-4e00-c489-21638374b497"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lowercase\n",
        "train_data = train_data.lower()\n",
        "val_data = val_data.lower()\n",
        "\n",
        "# remove punctuation, keep basic sentence delimiters (.!?)\n",
        "train_data = re.sub(r\"[^\\w\\s.?!]\", \"\", train_data)\n",
        "val_data = re.sub(r\"[^\\w\\s.?!]\", \"\", val_data)\n",
        "\n",
        "# tokenize by words\n",
        "train_tokens = word_tokenize(train_data)\n",
        "val_tokens = word_tokenize(val_data)\n",
        "\n",
        "# build vocab\n",
        "vocabulary = sorted(set(train_tokens))\n",
        "\n",
        "word_to_id = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "word_to_id[\"[UNK]\"] = len(word_to_id)\n",
        "vocab_size = len(word_to_id)\n",
        "\n",
        "id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
        "\n",
        "train_ids = [word_to_id.get(word, word_to_id[\"[UNK]\"]) for word in train_tokens]\n",
        "val_ids = [word_to_id.get(word, word_to_id[\"[UNK]\"]) for word in val_tokens]\n",
        "\n",
        "# X and y using sliding window len 3\n",
        "X_train = []\n",
        "y_train = []\n",
        "X_val = []\n",
        "y_val = []\n",
        "\n",
        "seq_len = 5\n",
        "\n",
        "for i in range(len(train_ids) - seq_len):\n",
        "      X_train.append(train_ids[i:i+seq_len])\n",
        "      y_train.append(train_ids[i+seq_len])\n",
        "\n",
        "for i in range(len(val_ids) - seq_len):\n",
        "      X_val.append(val_ids[i:i+seq_len])\n",
        "      y_val.append(val_ids[i+seq_len])\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_val = np.array(X_val)\n",
        "y_val = np.array(y_val)\n",
        "\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "RvXRFVcbMLe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c95e8b64-3d57-4756-ef0c-f8f48906066d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Embedding Layer in Keras\n",
        "Below is a minimal example of defining an `Embedding` layer:\n",
        "```python\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    input_dim=vocab_size,     # size of the vocabulary\n",
        "    output_dim=128,           # embedding vector dimension\n",
        "    input_length=sequence_length\n",
        ")\n",
        "```\n",
        "- This layer transforms integer-encoded sequences (word IDs) into dense vector embeddings.\n",
        "\n",
        "- Feed these embeddings into your LSTM or GRU OR 1D CNN layer."
      ],
      "metadata": {
        "id": "jbTZs3OiMMNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    input_dim= vocab_size,     # size of the vocabulary\n",
        "    output_dim= 128,                # embedding vector dimension\n",
        "    input_length= seq_len\n",
        ")"
      ],
      "metadata": {
        "id": "OXCK40l6MRld",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d45bbf4-04b8-41d6-e8d0-1633ae950f64"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model\n",
        "- Implement an LSTM or GRU or 1D CNN-based language model with:\n",
        "  - **The Embedding layer** as input.\n",
        "  - At least **one recurrent layer** (e.g., `LSTM(256)` or `GRU(256)` or your custom 1D CNN).\n",
        "  - A **Dense** output layer with **softmax** activation for word prediction.\n",
        "- Train for about **5–10 epochs** so it can finish in approximately **2 hours** on a standard machine.\n"
      ],
      "metadata": {
        "id": "qsXR4RZpMXMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dropout, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "es = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
        "\n",
        "model = Sequential([\n",
        "    embedding_layer,\n",
        "    LSTM(256),\n",
        "    Dropout(0.5),\n",
        "    # LSTM(256),\n",
        "    # Dropout(0.4),\n",
        "\n",
        "\n",
        "    #332\n",
        "    # GRU(256),\n",
        "    # Dropout(0.5),\n",
        "\n",
        "\n",
        "    # GRU(256),\n",
        "    # Dropout(0.3),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "lTKyufb_hhzD",
        "outputId": "182b2a81-2d7b-4b43-e5ce-15059c7e184f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training & Evaluation\n",
        "- **Monitor** the loss on both training and validation sets.\n",
        "- **Perplexity**: a common metric for language models.\n",
        "  - It is the exponent of the average negative log-likelihood.\n",
        "  - If your model outputs cross-entropy loss `H`, then `perplexity = e^H`.\n",
        "  - Try to keep the validation perplexity **under 50** if possible."
      ],
      "metadata": {
        "id": "Ggop4h4IMhMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network_history = model.fit(X_train, y_train,\n",
        "                            validation_data=(X_val,y_val),\n",
        "                            batch_size=64, #64, 318\n",
        "                            epochs=10,\n",
        "                            verbose=1,\n",
        "                            callbacks=[es])\n",
        "\n",
        "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
        "val_perplexity = np.exp(val_loss)\n",
        "\n",
        "print(\"Validation Perplexity: \", val_perplexity)"
      ],
      "metadata": {
        "id": "P8d8FS2XMj46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16f331ec-58aa-4e91-a4ea-f3805cb11687"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.0517 - loss: 6.3861 - val_accuracy: 0.0506 - val_loss: 5.8831\n",
            "Epoch 2/10\n",
            "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 52ms/step - accuracy: 0.0588 - loss: 5.6638 - val_accuracy: 0.0582 - val_loss: 5.8409\n",
            "Epoch 3/10\n",
            "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - accuracy: 0.0758 - loss: 5.4169 - val_accuracy: 0.0926 - val_loss: 5.7071\n",
            "Epoch 4/10\n",
            "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.1119 - loss: 5.1502 - val_accuracy: 0.1074 - val_loss: 5.5923\n",
            "Epoch 5/10\n",
            "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 52ms/step - accuracy: 0.1333 - loss: 4.8700 - val_accuracy: 0.1200 - val_loss: 5.4964\n",
            "Epoch 6/10\n",
            "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 53ms/step - accuracy: 0.1560 - loss: 4.6450 - val_accuracy: 0.1263 - val_loss: 5.4832\n",
            "Epoch 7/10\n",
            "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - accuracy: 0.1761 - loss: 4.4332 - val_accuracy: 0.1319 - val_loss: 5.4561\n",
            "Epoch 8/10\n",
            "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 53ms/step - accuracy: 0.1884 - loss: 4.2392 - val_accuracy: 0.1323 - val_loss: 5.5058\n",
            "Epoch 9/10\n",
            "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - accuracy: 0.2010 - loss: 4.1007 - val_accuracy: 0.1373 - val_loss: 5.5201\n",
            "Epoch 10/10\n",
            "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 54ms/step - accuracy: 0.2166 - loss: 3.9136 - val_accuracy: 0.1377 - val_loss: 5.5676\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.1217 - loss: 5.5205\n",
            "Validation Perplexity:  234.19327057231644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Generation Criteria\n",
        "- After training, generate **two distinct text samples**, each at least **50 tokens**.\n",
        "- Use **different seed phrases** (e.g., “love is” vs. “time will”)."
      ],
      "metadata": {
        "id": "cbvbBOp3MfTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function for generating text\n",
        "\n",
        "def generate_text(seed_phrase, model, word_to_id, seq_len=5, min_tokens=50):\n",
        "    # tokenize the seed phrase\n",
        "    tokens = word_tokenize(seed_phrase.lower())\n",
        "    token_ids = [word_to_id[token] if token in word_to_id else word_to_id[\"[UNK]\"] for token in tokens]\n",
        "\n",
        "    # padding to min sequence length\n",
        "    if len(token_ids) < seq_len:\n",
        "        token_ids = [word_to_id[\"[UNK]\"]] * (seq_len - len(token_ids)) + token_ids\n",
        "\n",
        "    # generate tokens\n",
        "    generated_tokens = tokens.copy()\n",
        "    while len(generated_tokens) < min_tokens:\n",
        "        # prepare input for the model\n",
        "        input_sequence = token_ids[-seq_len:]\n",
        "        input_sequence = np.array(input_sequence).reshape(1, -1)\n",
        "\n",
        "        # pwredict next word (token)\n",
        "        predicted_probs = model.predict(input_sequence, verbose=0)\n",
        "        predicted_id = np.random.choice(len(predicted_probs[0]), p=predicted_probs[0])\n",
        "\n",
        "        # map predicted ID to word\n",
        "        predicted_word = id_to_word[predicted_id]\n",
        "        generated_tokens.append(predicted_word)\n",
        "\n",
        "        # update token_ids for the next prediction\n",
        "        token_ids.append(predicted_id)\n",
        "\n",
        "    return ' '.join(generated_tokens)"
      ],
      "metadata": {
        "id": "1uHjn6aHMW5K"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\"love is\", model, word_to_id)"
      ],
      "metadata": {
        "id": "n5CpdqF9MoPj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0529ba44-0b17-48a7-d30c-44fa1cc825b3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'love is is coming . thats way you as a good just green left out of i said christopher robin read . dozen you kanga without so lines flying they could just and down to myself and i so asked christopher robin . that only baby pooh looking on it'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\"time will\", model, word_to_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "azby0gCUl1AW",
        "outputId": "d48f8477-a7ab-4e9b-a961-a599dcb8e65a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'time will you always found a heffalump without . i quite remember he long bees after bang . i didnt be something in a week . he was the top behind him of his moment . so please to say what whats the song . piglet ! is him .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}